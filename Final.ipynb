{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "import threading\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3063135473.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmy_model = \"C:\\Users\\jvsnn\\Videos\\Movavi Video Editor\\Projects\\my_model\"\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "model = keras.models.load_model(\"my_model\")\n",
    "y = np.load('landmarks/y.npz')['arr_0']\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_from_wrist(results, hand):\n",
    "    i = 0\n",
    "    landmarks = []\n",
    "    references = [0,0,0]\n",
    "    for cords in results.multi_hand_landmarks[0].landmark:\n",
    "        if i == 0:\n",
    "            references[0] = cords.x\n",
    "            references[1] = cords.y\n",
    "            references[2] = cords.z\n",
    "            landmarks.append([0,0,0])\n",
    "\n",
    "        else:\n",
    "            if hand == 'Right':\n",
    "                landmarks.append([cords.x - references[0],cords.y - references[1],cords.z])\n",
    "            else:\n",
    "                landmarks.append([references[0] - cords.x,cords.y - references[1],cords.z])\n",
    "        i += 1\n",
    "    return landmarks\n",
    "\n",
    "def handsign_prediction(landmarks, show=True):\n",
    "    pred = model.predict([landmarks], batch_size=1)[0]\n",
    "    prob = pred[np.argmax(pred)]\n",
    "    return le.classes_[np.argmax(pred)],prob\n",
    "\n",
    "def text_to_speech(*text):\n",
    "    text = ''.join(text)\n",
    "    text_speech = gTTS(text=text, lang = 'en')\n",
    "    filename = 'text_to_speech.mp3'\n",
    "    text_speech.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "    os.remove(filename)\n",
    "    return\n",
    "\n",
    "def draw_censor(results, image):\n",
    "    smallest_x = 0\n",
    "    smallest_y = 0\n",
    "    biggest_x = 0\n",
    "    biggest_y = 0\n",
    "    i = 0\n",
    "\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    if not results.multi_hand_landmarks == None:\n",
    "        for cords in results.multi_hand_landmarks[0].landmark:\n",
    "            x = cords.x * image_width\n",
    "            y = cords.y * image_height\n",
    "            if i == 0:\n",
    "                smallest_x = int(x)\n",
    "                smallest_y = int(y)\n",
    "                biggest_x = int(x)\n",
    "                biggest_y = int(y)\n",
    "            else:\n",
    "                if x < smallest_x:\n",
    "                    smallest_x = int(x)\n",
    "                if y < smallest_y:\n",
    "                    smallest_y = int(y)\n",
    "                if x > biggest_x:\n",
    "                    biggest_x = int(x)\n",
    "                if y > biggest_y:\n",
    "                    biggest_y = int(y)\n",
    "            i += 1\n",
    "\n",
    "    image = cv2.rectangle(image,(smallest_x-50, smallest_y-50), (biggest_x+50, biggest_y), (0,0,0),-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results.multi_hand_landmarks:\n\u001b[32m     40\u001b[39m         mp_drawing.draw_landmarks(\n\u001b[32m     41\u001b[39m             annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     pred, prob = \u001b[43mhandsign_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_landmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     pred = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mhandsign_prediction\u001b[39m\u001b[34m(landmarks, show)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhandsign_prediction\u001b[39m(landmarks, show=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     pred = \u001b[43mmodel\u001b[49m.predict([landmarks], batch_size=\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m     prob = pred[np.argmax(pred)]\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m le.classes_[np.argmax(pred)],prob\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word = ''\n",
    "current_word = ''\n",
    "STORE_FRAME = 8\n",
    "frame = 0\n",
    "pred = ''\n",
    "prob = 0\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.8,\n",
    "    min_tracking_confidence=0.8) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        annotated_image = image.copy()\n",
    "        censor_image = image.copy()\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand = results.multi_handedness[0].classification[0].label\n",
    "            new_landmarks = reference_from_wrist(results, hand)\n",
    "            \n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            pred, prob = handsign_prediction(new_landmarks, show = False)\n",
    "        else:\n",
    "            pred = ''\n",
    "            prob = 0\n",
    "        # describe the type of font \n",
    "        # to be used. \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        prediction = 'Prediction: '+ str(pred)\n",
    "        probability = 'Probability: '+ str(prob)\n",
    "        # Use putText() method for \n",
    "        # inserting text on video \n",
    "        if prob > 0.6:\n",
    "            cv2.putText(image,  \n",
    "                        probability,  \n",
    "                        (50, 80),  \n",
    "                        font, 0.75,  \n",
    "                        (0, 255, 0),  \n",
    "                        2,  \n",
    "                        cv2.LINE_4) \n",
    "            if pred == 'middle_finger':\n",
    "                image = draw_censor(results, censor_image)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(image,  \n",
    "                    prediction,  \n",
    "                    (50, 50),  \n",
    "                    font, 0.75,  \n",
    "                    (0, 255, 0),  \n",
    "                    2,  \n",
    "                    cv2.LINE_4)\n",
    "                if current_word != pred:\n",
    "                    frame = 0\n",
    "                    current_word = pred\n",
    "                else:\n",
    "                    frame += 1\n",
    "\n",
    "                if frame == STORE_FRAME:\n",
    "                    if current_word == 'space':\n",
    "                        speech = threading.Thread(target=text_to_speech, name=\"speech\", args=word)\n",
    "                        speech.start()\n",
    "                        word = ''\n",
    "                        current_word = ''\n",
    "                        frame = 0\n",
    "                    else:\n",
    "                        word += current_word\n",
    "                        current_word = ''\n",
    "                            \n",
    "                \n",
    "        cv2.putText(image,  \n",
    "                    word,  \n",
    "                    (50, 110),  \n",
    "                    font, 0.75,  \n",
    "                    (0, 255, 0),  \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
